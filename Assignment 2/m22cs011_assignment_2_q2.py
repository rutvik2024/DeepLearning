# -*- coding: utf-8 -*-
"""M22CS011_ASSIGNMENT_2_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oIA_AHJfwnMa5Lr6HS-QJ1Jia3WFYXDe

# Importing the libraries
"""

import torch
import numpy as np
import torch.nn as nn
import torch.nn.functional as func
from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split
import io
import os
import zipfile
import random 
import math
import unicodedata
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

"""# Load File in Drive"""

!wget 'http://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip'

!unzip 'household_power_consumption.zip'

"""# Loading the dataset"""

path = '/content/household_power_consumption.txt'
df = pd.read_csv(path, delimiter=';', 
                  infer_datetime_format=True, 
                 na_values=['nan','?'])

df.head()

df.dropna(inplace=True)

# Extract the input features and target variable from the dataframe
X = df.iloc[:,3:].values
y = df.iloc[:, 2:3].values

"""# Dataset Split"""

def splitDataset(X, y, test_size):
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)

  return X_train, X_test, y_train, y_test

"""# Convert dataset to tensor"""

def convertToTonsor(x):
  return torch.tensor(x, dtype=torch.float32)

"""# Hyperparamater"""

input_size = X.shape[1]
hidden_size = 64
num_layers = 2
output_size = 1
learning_rate = 0.0001
batch_size = 32
num_epochs = 10

"""# for 80:20

## Split dataset 80:20
"""

# Split dataset into 80:20
X_train, X_test, y_train, y_test = splitDataset(X, y, 0.2)
print(type(X_train), len(X_train))
print(type(X_test), len(X_test))
print(type(y_train), len(y_train))
print(type(y_test), len(y_test))


# Convert the input features and target variable into PyTorch tensors

X_train = convertToTonsor(X_train)
print(type(X_train), len(X_train))


y_train = convertToTonsor(y_train)
print(type(y_train), len(y_train))

X_test = convertToTonsor(X_test)
print(type(X_test), len(X_test))

y_test = convertToTonsor(y_test)
print(type(y_test), len(y_test))

"""## Create dataset for 80:20"""

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

print("Len of train : ",len(train_dataset))
print("Len of test : ",len(test_dataset))

"""## data preprocessing for 80:20"""

def msn(x):
  x_mean = x.mean(dim=0)
  x_std  = x.std(dim=0)
  return x_mean, x_std, (x - x_mean) / x_std

X_train_mean, X_train_std, X_train_norm = msn(X_train)

print(X_train_mean)
print(X_train_std)
print(X_train_norm)

X_test_mean, X_test_std, X_test_norm = msn(X_test)

print(X_test_mean)
print(X_test_std)
print(X_test_norm)

y_train_mean, y_train_std, y_train_norm = msn(y_train)

print(y_train_mean)
print(y_train_std)
print(y_train_norm)

y_test_mean, y_test_std, y_test_norm = msn(y_test)

print(y_test_mean)
print(y_test_std)
print(y_test_norm)

"""## dataloader for 80:20"""

# Create training dataloader
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Create testing dataloader
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""# Cuda device"""

device= 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""# LSTM Model"""

class CustomLSTM(nn.Module):
    def __init__(self, input_sz, hidden_sz):
        super().__init__()
        self.input_sz = input_sz
        self.hidden_size = hidden_sz
        self.W = nn.Parameter(torch.Tensor(input_sz, hidden_sz * 4))
        self.U = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz * 4))
        self.bias = nn.Parameter(torch.Tensor(hidden_sz * 4))
        self.init_weights()
        self.fc = nn.Linear(hidden_size, output_size)
                
    def init_weights(self):
        stdv = 1.0 / math.sqrt(self.hidden_size)
        for weight in self.parameters():
            weight.data.uniform_(-stdv, stdv)
         
    def forward(self, x, init_states=None):
        """Assumes x is of shape (batch, sequence, feature)"""
        bs, seq_sz = x.size()
        hidden_seq = []
        if init_states is None:
            h_t, c_t = (torch.zeros(bs, self.hidden_size).to(x.device), 
                        torch.zeros(bs, self.hidden_size).to(x.device))
        else:
            h_t, c_t = init_states
         
        HS = self.hidden_size
        for t in range(seq_sz):
            # print(x.shape)
            x_t = x[t, :]
            # batch the computations into a single matrix multiplication
            gates = x_t @ self.W + h_t @ self.U + self.bias
            i_t, f_t, g_t, o_t = (
                torch.sigmoid(gates[:, :HS]), # input
                torch.sigmoid(gates[:, HS:HS*2]), # forget
                torch.tanh(gates[:, HS*2:HS*3]),
                torch.sigmoid(gates[:, HS*3:]), # output
            )
            c_t = f_t * c_t + i_t * g_t
            h_t = o_t * torch.tanh(c_t)
            hidden_seq.append(h_t.unsqueeze(0))
        hidden_seq = torch.cat(hidden_seq, dim=0)
        # reshape from shape (sequence, batch, feature) to (batch, sequence, feature)
        hidden_seq = hidden_seq.transpose(0, 1).contiguous()
        hidden_seq, (h_t, c_t)

        hidden_seq = self.fc(hidden_seq[:, -1, :])
        return hidden_seq

# Initialize the model, loss function, and optimizer
model = CustomLSTM(input_size, hidden_size)

"""# Define the loss function and optimizer"""

criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

"""# Train the model"""

train_loss_list = []

for epoch in range(num_epochs):
    train_loss = 0.0
    for i, (inputs, targets) in enumerate(train_dataloader):
        inputs = (inputs - X_train_mean) / X_train_std # normalize the inputs

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

        if (i + 1) % batch_size == 0: # Update the model after every batch_size iterations
            optimizer.step()
            optimizer.zero_grad()

        train_loss /= len(train_dataloader)
    train_loss_list.append(train_loss)
    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_dataloader), loss.item()))

print(len(train_loss_list))

"""# Plot the loss"""

epoch = [epoch for epoch in range(num_epochs)]
plt.plot(epoch,train_loss_list, color='magenta', marker='o',mfc='pink' )
plt.ylabel('train_loss_82') #set the label for x-axis
plt.xlabel('no_of_epochs') #set the label for y axis
plt.title("Plotting a list") #set the title of the graph
plt.show() #display the graph

"""# Test the model and get the predicted value"""

# Set model to evaluation mode
model.eval()

# Initialize lists to store real and predicted global active power
real_global_active_power = []
predicted_global_active_power = []

# Iterate over the testing dataloader
for i, (inputs, targets) in enumerate(test_dataloader):
    inputs = (inputs - X_test_mean) / X_test_std # normalize the inputs

    # Forward pass to get predicted global active power
    outputs = model(inputs)
    predicted_global_active_power.extend(outputs.detach().numpy().tolist())

    # Append real global active power to list
    real_global_active_power.extend(targets.numpy().tolist())

# Convert the lists to numpy arrays for plotting
real_global_active_power = np.array(real_global_active_power)
predicted_global_active_power = np.array(predicted_global_active_power)

# Plot the real and predicted global active power
plt.figure(figsize=(10, 5))
plt.plot(real_global_active_power, label='Real Global Active Power')
plt.plot(predicted_global_active_power, label='Predicted Global Active Power')
plt.xlabel('Testing Days')
plt.ylabel('Global Active Power_82')
plt.legend()
plt.title('Real vs Predicted Global Active Power')
plt.show()

"""# For 70:30

## Splitting into 70:30
"""

# Split dataset into 70:30
X_train, X_test, y_train, y_test = splitDataset(X, y, 0.3)
print(type(X_train), len(X_train))
print(type(X_test), len(X_test))
print(type(y_train), len(y_train))
print(type(y_test), len(y_test))


# Convert the input features and target variable into PyTorch tensors

X_train = convertToTonsor(X_train)
print(type(X_train), len(X_train))


y_train = convertToTonsor(y_train)
print(type(y_train), len(y_train))

X_test = convertToTonsor(X_test)
print(type(X_test), len(X_test))

y_test = convertToTonsor(y_test)
print(type(y_test), len(y_test))

"""## Create dataset for 70:30"""

train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)

print("Len of train : ",len(train_dataset))
print("Len of test : ",len(test_dataset))

"""## Data preprocessing for 70;30"""

def msn(x):
  x_mean = x.mean(dim=0)
  x_std  = x.std(dim=0)
  return x_mean, x_std, (x - x_mean) / x_std

X_train_mean, X_train_std, X_train_norm = msn(X_train)

print(X_train_mean)
print(X_train_std)
print(X_train_norm)

X_test_mean, X_test_std, X_test_norm = msn(X_test)

print(X_test_mean)
print(X_test_std)
print(X_test_norm)

y_train_mean, y_train_std, y_train_norm = msn(y_train)

print(y_train_mean)
print(y_train_std)
print(y_train_norm)

y_test_mean, y_test_std, y_test_norm = msn(y_test)

print(y_test_mean)
print(y_test_std)
print(y_test_norm)

"""## Dataloader for 70:30"""

# Create training dataloader
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Create testing dataloader
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

"""# Train the model"""

model = CustomLSTM(input_size, hidden_size)

train_loss_list = []

for epoch in range(num_epochs):
    train_loss = 0.0
    for i, (inputs, targets) in enumerate(train_dataloader):
        inputs = (inputs - X_test_mean) / X_test_std # normalize the inputs

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

        if (i + 1) % batch_size == 0: # Update the model after every batch_size iterations
            optimizer.step()
            optimizer.zero_grad()

        train_loss /= len(train_dataloader)
    train_loss_list.append(train_loss)
    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_dataloader), loss.item()))

"""# Plot the loss"""

epoch = [epoch for epoch in range(num_epochs)]
plt.plot(epoch,train_loss_list, color='magenta', marker='o',mfc='pink' )
plt.ylabel('train_loss_7_3') #set the label for x-axis
plt.xlabel('no_of_epochs') #set the label for y axis
plt.title("Plotting a list") #set the title of the graph
plt.show() #display the graph

"""# Test the model and get the predicted model

"""

import matplotlib.pyplot as plt

# Set model to evaluation mode
model.eval()

# Initialize lists to store real and predicted global active power
real_global_active_power = []
predicted_global_active_power = []

# Iterate over the testing dataloader
for i, (inputs, targets) in enumerate(test_dataloader):
    inputs = (inputs - X_test_mean) / X_test_std # normalize the inputs

    # Forward pass to get predicted global active power
    outputs = model(inputs)
    predicted_global_active_power.extend(outputs.detach().numpy().tolist())

    # Append real global active power to list
    real_global_active_power.extend(targets.numpy().tolist())

# Convert the lists to numpy arrays for plotting
real_global_active_power = np.array(real_global_active_power)
predicted_global_active_power = np.array(predicted_global_active_power)

# Plot the real and predicted global active power
plt.figure(figsize=(10, 5))
plt.plot(real_global_active_power, label='Real Global Active Power')
plt.plot(predicted_global_active_power, label='Predicted Global Active Power')
plt.xlabel('Testing Days')
plt.ylabel('Global Active Power_73')
plt.legend()
plt.title('Real vs Predicted Global Active Power')
plt.show()

