{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dffba56367254aa78c4625db9491ec5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bac71f715bae4a75a84fba68a1b62a70",
              "IPY_MODEL_df2a846d084c400391287c07f1256bc6",
              "IPY_MODEL_2e6e66704e4a433faaadf82d4fa9d787"
            ],
            "layout": "IPY_MODEL_6a71b8c7020747b89e3fcc06b1e44908"
          }
        },
        "bac71f715bae4a75a84fba68a1b62a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee602c69f9c441a79f6c1a95f4fe6983",
            "placeholder": "​",
            "style": "IPY_MODEL_1587717e10f74a6ab9a1b7949fda8857",
            "value": " 60%"
          }
        },
        "df2a846d084c400391287c07f1256bc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff40bd4737d24b749fa4ea0080c1cc0b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51a02383c10149b9adba36714bed1986",
            "value": 3
          }
        },
        "2e6e66704e4a433faaadf82d4fa9d787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a453770a6774c77a1b8a978c6b92400",
            "placeholder": "​",
            "style": "IPY_MODEL_647f2f1c37494dd7b26cd688ecc8b08a",
            "value": " 3/5 [06:06&lt;03:04, 92.01s/it]"
          }
        },
        "6a71b8c7020747b89e3fcc06b1e44908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee602c69f9c441a79f6c1a95f4fe6983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1587717e10f74a6ab9a1b7949fda8857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff40bd4737d24b749fa4ea0080c1cc0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51a02383c10149b9adba36714bed1986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1a453770a6774c77a1b8a978c6b92400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "647f2f1c37494dd7b26cd688ecc8b08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPfoQUqzYlne",
        "outputId": "340f4508-9397-49d6-8ade-5ab05d347253"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 28 14:41:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0    34W /  70W |   2221MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "iuqNCPhSo1xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import urllib\n",
        "import zipfile\n",
        "from tqdm.auto import tqdm\n"
      ],
      "metadata": {
        "id": "1u7Gfk9no6GA"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Config"
      ],
      "metadata": {
        "id": "K-8r-Z9WsCeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-o7YI_JtsGE6",
        "outputId": "a5ef0c15-840e-4eb1-98b9-8cee7a87cce8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParameter"
      ],
      "metadata": {
        "id": "bc2rvqWRzDXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "input_channel = 3 # RGB\n",
        "n_filters = 12\n",
        "kernel_size = 3 # filter size of 3*3\n",
        "output = 200\n",
        "n_iters = 5\n",
        "learning_rate = 0.001\n",
        "temperature = 4\n",
        "alpha = 0.999\n",
        "decay=0.999"
      ],
      "metadata": {
        "id": "FWPNPrmMzHS3"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Extract Dataset"
      ],
      "metadata": {
        "id": "AjJfhZx7teqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "\n",
        "!unzip -qq 'tiny-imagenet-200.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OW4PyymUsPVI",
        "outputId": "934d73ab-875a-4e29-e8f8-c2af769ca003"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-28 14:41:28--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.68.10\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip.1’\n",
            "\n",
            "tiny-imagenet-200.z  37%[======>             ]  88.08M  15.2MB/s    eta 17s    ^C\n",
            "replace tiny-imagenet-200/words.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Folder path"
      ],
      "metadata": {
        "id": "isnL8p-Gttv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/tiny-imagenet-200'\n",
        "\n",
        "# Define training and validation data paths\n",
        "train_dir = os.path.join(data_dir, 'train') \n",
        "test_dir = os.path.join(data_dir, 'test') \n",
        "val_dir = os.path.join(data_dir, 'val')\n"
      ],
      "metadata": {
        "id": "1hE_YAEItlav"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and DataLoader"
      ],
      "metadata": {
        "id": "oxJgHshfxqo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(train_dir,transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "test_dataset = torchvision.datasets.ImageFolder(test_dir,transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "val_dataset = torchvision.datasets.ImageFolder(val_dir,transform=transforms.ToTensor())\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "xINge9_tvbZd"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loader Varification"
      ],
      "metadata": {
        "id": "MXe5vi2yxuhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(train_loader))\n",
        "print(image.shape)\n",
        "print(label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rULjhPY_zZHg",
        "outputId": "d01c05b9-fa2f-4d7e-caa9-f773f551f3d5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 64, 64])\n",
            "torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# teacher Implementation"
      ],
      "metadata": {
        "id": "nHFfV6lO0HoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class teacherNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(teacherNet, self).__init__()\n",
        "    self.cv1 = nn.Conv2d(in_channels= 3, out_channels= 12, kernel_size= 5, padding= 1, stride=1)\n",
        "    self.cv2 = nn.Conv2d(in_channels= 12, out_channels = 24, kernel_size= 5, padding=1, stride = 1)\n",
        "    self.cv3 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv4 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv5 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv6 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv7 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv8 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv9 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv10 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv11 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.cv12 = nn.Conv2d(in_channels= 24, out_channels= 24, kernel_size=5, padding=1, stride=1)\n",
        "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(24*20*20, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 200)\n",
        "\n",
        "\n",
        "    # xavier weight initialization\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(m.weight).to(device)\n",
        "\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight).to(device)\n",
        "\n",
        "      \n",
        "  def forward(self,x):\n",
        "    x = self.cv1(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 1\",x.shape)\n",
        "\n",
        "    x = self.cv2(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 2\",x.shape)\n",
        "\n",
        "    x = self.cv3(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 3\",x.shape)\n",
        "\n",
        "    x = self.cv4(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 4\",x.shape)\n",
        "\n",
        "    x = self.cv5(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 5\",x.shape)\n",
        "\n",
        "    x = self.cv6(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 6\",x.shape)\n",
        "\n",
        "    x = self.cv7(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 7\",x.shape)\n",
        "\n",
        "    x = self.cv8(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 8\",x.shape)\n",
        "\n",
        "    x = self.cv9(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 9\",x.shape)\n",
        "\n",
        "    x = self.cv10(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 10\",x.shape)\n",
        "\n",
        "    x = self.cv11(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 11\",x.shape)\n",
        "\n",
        "    x = self.cv12(x)\n",
        "    x = self.tanh(x)\n",
        "    x = self.pool(x)\n",
        "    # print(\"cnv 12\",x.shape)\n",
        "\n",
        "    # x = x.view(x.size(0), -1)\n",
        "    x = self.flat(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"Fc1 : \",x.shape)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    # print(x.shape)\n",
        "    return x    \n",
        "\n",
        "\n",
        "\n",
        "teacher_model = teacherNet()\n",
        "teacher_model.to(device)\n",
        "teacher_model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_g8mVudziSn",
        "outputId": "5cf77a14-5b41-40f4-9ecb-2b09e0b5f259"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "teacherNet(\n",
              "  (cv1): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv2): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv3): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv4): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv5): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv6): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv7): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv8): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv9): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv10): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv11): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv12): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  (tanh): Tanh()\n",
              "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=9600, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Student Model implementation"
      ],
      "metadata": {
        "id": "JTsxk7mlneUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class studentNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(studentNet, self).__init__()\n",
        "    self.cv1 = nn.Conv2d(in_channels = 3, out_channels=12, kernel_size = 5, stride=1, padding=1)\n",
        "    self.cv2 = nn.Conv2d(in_channels = 12, out_channels=24, kernel_size = 5, stride=1, padding=1)\n",
        "    self.cv3 = nn.Conv2d(in_channels = 24, out_channels=24, kernel_size = 5, stride=1, padding=1)\n",
        "    self.cv4 = nn.Conv2d(in_channels = 24, out_channels=24, kernel_size = 5, stride=1, padding=1)\n",
        "    self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.flat = nn.Flatten()\n",
        "    self.fc1 = nn.Linear(24*28*28, 1024)\n",
        "    self.fc2 = nn.Linear(1024, 200)\n",
        "\n",
        "\n",
        "    # xavier weight initialization\n",
        "    for m in self.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "          nn.init.xavier_uniform_(m.weight).to(device)\n",
        "\n",
        "        elif isinstance(m, nn.Linear):\n",
        "          nn.init.xavier_uniform_(m.weight).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cv1(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 1\",x.shape)\n",
        "\n",
        "    x = self.cv2(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 2\",x.shape)\n",
        "\n",
        "    x = self.cv3(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"cnv 3\",x.shape)\n",
        "\n",
        "    x = self.cv4(x)\n",
        "    x = self.tanh(x)\n",
        "    x = self.pool(x)\n",
        "    # print(\"cnv 12\",x.shape)\n",
        "\n",
        "    # x = x.view(x.size(0), -1)\n",
        "    x = self.flat(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.tanh(x)\n",
        "    # print(\"Fc1 : \",x.shape)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    # print(\"Fc2 : \",x.shape)\n",
        "    return x    \n",
        "\n",
        "student_model = studentNet().to(device)\n",
        "student_model"
      ],
      "metadata": {
        "id": "gvbx4O1LnhpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf746a5-8dda-439a-d6d2-12d6c80a1353"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "studentNet(\n",
              "  (cv1): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv2): Conv2d(12, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv3): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (cv4): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "  (tanh): Tanh()\n",
              "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
              "  (fc1): Linear(in_features=18816, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# xaviour weight initialization"
      ],
      "metadata": {
        "id": "l_1JHiuM0SlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.init.xavier_uniform_(model.conv1.weight)\n",
        "# nn.init.xavier_uniform_(model.conv2.weight)\n",
        "# nn.init.xavier_uniform_(model.conv3.weight)\n",
        "# nn.init.xavier_uniform_(model.conv4.weight)\n",
        "# nn.init.xavier_uniform_(model.conv5.weight)\n",
        "# nn.init.xavier_uniform_(model.conv6.weight)\n",
        "# nn.init.xavier_uniform_(model.fc1.weight)\n",
        "# nn.init.xavier_uniform_(model.fc2.weight)"
      ],
      "metadata": {
        "id": "tm61CIT50LUX"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def weight_init(m):\n",
        "#   if isinstance(m, nn.Conv2d):\n",
        "#     nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('tanh'))\n",
        "\n",
        "# model2 = ConvNet()\n",
        "# model2.apply(weight_init)"
      ],
      "metadata": {
        "id": "Z3xs4MEe0a_S"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model2.conv1.weight)\n",
        "# print(model2.conv2.weight)\n",
        "# print(model2.conv3.weight)\n",
        "# print(model2.conv4.weight)\n",
        "# print(model2.conv5.weight)\n",
        "# print(model2.conv6.weight)  "
      ],
      "metadata": {
        "id": "aGoZFQ4ee8pe"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss and Optimizer"
      ],
      "metadata": {
        "id": "su2pflBIGlUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fun = nn.CrossEntropyLoss()\n",
        "loss_fun"
      ],
      "metadata": {
        "id": "Oc_e4mP2kqcl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b1baa4-ed1c-4d7c-b189-f44f91e03a32"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_teacher = torch.optim.SGD(teacher_model.parameters(), lr= learning_rate)\n",
        "optimizer_teacher\n",
        "\n",
        "optimizer_student = torch.optim.SGD(student_model.parameters(), lr= learning_rate)\n",
        "optimizer_student"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrIOTg6HGuNl",
        "outputId": "c92d94d5-6088-4aa0-ce1e-7ea18230f0f4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Plot Code"
      ],
      "metadata": {
        "id": "dyk-_vhfGoS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = []\n",
        "no_of_epochs=n_iters\n",
        "for i in range(no_of_epochs):\n",
        "  epoch.append(i+1)\n",
        "\n",
        "def plot_graph(y):\n",
        "  plt.plot(epoch,y)\n",
        "  plt.show() #display the graph"
      ],
      "metadata": {
        "id": "QPMwspdiGrL8"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Without EMA"
      ],
      "metadata": {
        "id": "oEpFyXZFEbng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop for teacher without EMA"
      ],
      "metadata": {
        "id": "s2wbTpykHAke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_steps = len(train_loader)\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "for epoch in tqdm(range(n_iters)):\n",
        "  total_samples=0\n",
        "  correct_samples=0\n",
        "  \n",
        "  for step, (image, label) in enumerate(train_loader):\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = teacher_model(image)\n",
        "    # print(output.shape)\n",
        "    # print(label.shape)\n",
        "    # loss\n",
        "    loss = loss_fun(output, label)\n",
        "\n",
        "\n",
        "    _,out=torch.max(output,1)\n",
        "    total_samples+=label.shape[0]\n",
        "    correct_samples+=(out==label).sum().item()\n",
        "    \n",
        "    #backward pass\n",
        "    optimizer_teacher.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_teacher.step()\n",
        "\n",
        "\n",
        "  accuracy=(correct_samples/total_samples)*100.0\n",
        "  print(\"Epoch: {}/{} | step:{}/{} | Loss:{:.4f} | Accuracy{: .2f}\".format(epoch,n_iters,step,total_steps,loss.item(),accuracy))\n",
        "  train_loss_list.append(loss.item())\n",
        "  train_acc_list.append(accuracy)\n",
        "\n",
        "  # Test Loop \n",
        "  with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    for image, label in test_loader:\n",
        "      image = image.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      output = teacher_model(image)\n",
        "\n",
        "      # value, index\n",
        "      _, predictions = torch.max(output, 1)\n",
        "      n_samples += label.shape[0]\n",
        "      n_correct += (predictions == label).sum().item()\n",
        "      # print(n_correct)\n",
        "\n",
        "\n",
        "    acc = 100.0 * (n_correct/n_samples)\n",
        "    test_acc_list.append(acc)\n",
        "  print(\"Accuracy : \",acc)\n",
        "\n",
        "    \n",
        "# print(\"\\n\\nloss list\",list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371,
          "referenced_widgets": [
            "dffba56367254aa78c4625db9491ec5f",
            "bac71f715bae4a75a84fba68a1b62a70",
            "df2a846d084c400391287c07f1256bc6",
            "2e6e66704e4a433faaadf82d4fa9d787",
            "6a71b8c7020747b89e3fcc06b1e44908",
            "ee602c69f9c441a79f6c1a95f4fe6983",
            "1587717e10f74a6ab9a1b7949fda8857",
            "ff40bd4737d24b749fa4ea0080c1cc0b",
            "51a02383c10149b9adba36714bed1986",
            "1a453770a6774c77a1b8a978c6b92400",
            "647f2f1c37494dd7b26cd688ecc8b08a"
          ]
        },
        "id": "BXBMf4yLHD90",
        "outputId": "4d498d74-d1ae-4a09-e5b3-76effdc6aa8e"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dffba56367254aa78c4625db9491ec5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0/5 | step:1562/1563 | Loss:0.0909 | Accuracy 71.89\n",
            "Accuracy :  0.0\n",
            "Epoch: 1/5 | step:1562/1563 | Loss:0.0681 | Accuracy 72.91\n",
            "Accuracy :  0.0\n",
            "Epoch: 2/5 | step:1562/1563 | Loss:0.0812 | Accuracy 73.82\n",
            "Accuracy :  0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-afa024a2f08f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtotal_samples\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mcorrect_samples\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(train_loss_list)\n",
        "plot_graph(train_acc_list)\n",
        "plot_graph(test_acc_list)\n"
      ],
      "metadata": {
        "id": "HrtGI2N1GTBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## student model without EMA"
      ],
      "metadata": {
        "id": "Bf78ECXtes7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "scheduler = StepLR(optimizer_student, step_size=30, gamma=0.1)\n",
        "\n",
        "total_steps = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []  \n",
        "T = 3\n",
        "for epoch in tqdm(range(n_iters)):\n",
        "  total_samples=0\n",
        "  correct_samples=0\n",
        "  running_loss = 0.0\n",
        "  for step, (image, label) in enumerate(train_loader):\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "    optimizer_teacher.zero_grad()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      teacher_pred = teacher_model(image)\n",
        "    # print(output.shape)\n",
        "    # print(label.shape)\n",
        "    # loss\n",
        "    student_pred = student_model(image)\n",
        "\n",
        "    # print(type(student_pred))\n",
        "    # print(type(teacher_pred))\n",
        "    student_loss = loss_fun(student_pred, label)\n",
        "\n",
        "\n",
        "    # dest_loss = destillation_loss(student_pred, teacher_pred)\n",
        "    soft_teacher_pred = F.softmax(teacher_pred / temperature, dim=1)\n",
        "    # print(\"Soft teacher : \", soft_teacher_pred)\n",
        "    soft_student_pred = F.softmax(student_pred / temperature, dim=1)\n",
        "    # print(\"Soft student : \", soft_student_pred)\n",
        "    # dest_loss = F.kl_div(soft_teacher_pred.log(), soft_student_pred, reduction='batchmean') * temperature * temperature\n",
        "\n",
        "    dest_loss = nn.KLDivLoss()(F.log_softmax(soft_teacher_pred/T, dim=1),F.softmax(soft_student_pred/T, dim=1)) * T * T \n",
        "    # print(dest_loss)\n",
        "\n",
        "\n",
        "    # print(dest_loss, type(dest_loss))\n",
        "\n",
        "    loss = alpha * student_loss + (1 - alpha) * dest_loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    for param in student_model.parameters():\n",
        "        param.grad *= (temperature ** 2)\n",
        "\n",
        "\n",
        "    optimizer_student.step()\n",
        "\n",
        "    _,out=torch.max(student_pred,1)\n",
        "    total_samples+=label.shape[0]\n",
        "    correct_samples+=(out==label).sum().item()\n",
        "    # break\n",
        "\n",
        "  accuracy=(correct_samples/total_samples)*100.0\n",
        "  print(\"Epoch: {}/{} | step:{}/{} | Loss:{:.4f} | Accuracy{: .2f}\".format(epoch,n_iters,step,total_steps,loss.item(),accuracy))\n",
        "  loss_list.append(loss.item())\n",
        "  \n",
        "  # update learning rate\n",
        "  scheduler.step()\n",
        "  # validate model\n",
        "  student_model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "          outputs = student_model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  acc = 100 * correct / total\n",
        "  print('Test : Epoch %d: Loss = %.3f, Test Accuracy = %.3f %%' %\n",
        "        (epoch + 1, running_loss / len(train_loader), acc))\n",
        "  test_acc_list.append(acc)\n",
        "  test_loss_list.append(running_loss)\n",
        "\n",
        "    \n",
        "# print(\"\\n\\nloss list\",list)"
      ],
      "metadata": {
        "id": "3Uc-Wr50fp1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(loss_list)\n",
        "plot_graph(acc_list)\n",
        "plot_graph(test_loss_list)\n",
        "plot_graph(test_acc_list)\n"
      ],
      "metadata": {
        "id": "FFe0H490HNAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.KLDivLoss()\n",
        "optimizer = torch.optim.SGD(student_model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)"
      ],
      "metadata": {
        "id": "0G-MSKxg0DgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model With EMA"
      ],
      "metadata": {
        "id": "9QZcOMNTBLF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the EMA model\n",
        "class EMA:\n",
        "    def __init__(self, model, decay):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
        "                self.shadow[name] = new_average.clone()\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                assert name in self.shadow\n",
        "                param.data = self.shadow[name]\n"
      ],
      "metadata": {
        "id": "hOS8QaMT8qXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teacher model with EMA"
      ],
      "metadata": {
        "id": "78dj3vZ6CHav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []\n",
        "total_steps = len(train_loader)\n",
        "ema = EMA(decay = decay, model = teacher_model)\n",
        "ema.register()\n",
        "for epoch in tqdm(range(n_iters)):\n",
        "  total_samples=0\n",
        "  correct_samples=0\n",
        "  \n",
        "  for step, (image, label) in enumerate(train_loader):\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = teacher_model(image)\n",
        "    loss = loss_fun(output, label)\n",
        "\n",
        "\n",
        "    _,out=torch.max(output,1)\n",
        "    total_samples+=label.shape[0]\n",
        "    correct_samples+=(out==label).sum().item()\n",
        "    \n",
        "    #backward pass\n",
        "    optimizer_teacher.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_teacher.step()\n",
        "    ema.update()\n",
        "\n",
        "\n",
        "  accuracy=(correct_samples/total_samples)*100.0\n",
        "  print(\"Epoch: {}/{} | step:{}/{} | Loss:{:.4f} | Accuracy{: .2f}\".format(epoch,n_iters,step,total_steps,loss.item(),accuracy))\n",
        "  loss_list.append(loss.item())\n",
        "\n",
        "  scheduler.step()\n",
        "  # Test Loop \n",
        "  with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    for image, label in test_loader:\n",
        "      image = image.to(device)\n",
        "      label = label.to(device)\n",
        "\n",
        "      output = teacher_model(image)\n",
        "\n",
        "      # value, index\n",
        "      _, predictions = torch.max(output, 1)\n",
        "      n_samples += label.shape[0]\n",
        "      n_correct += (predictions == label).sum().item()\n",
        "      # print(n_correct)\n",
        "\n",
        "\n",
        "    acc = 100.0 * (n_correct/n_samples)\n",
        "\n",
        "print(\"Accuracy : \",acc)\n",
        "\n",
        "    \n",
        "# print(\"\\n\\nloss list\",list)"
      ],
      "metadata": {
        "id": "wBTqZJAhB4qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(train_loss_list)\n",
        "plot_graph(train_acc_list)\n",
        "plot_graph(test_acc_list)\n"
      ],
      "metadata": {
        "id": "thdN1FDZHa0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Student model with EMA"
      ],
      "metadata": {
        "id": "jjsNaqgACu99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "scheduler = StepLR(optimizer_student, step_size=30, gamma=0.1)\n",
        "\n",
        "total_steps = len(train_loader)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "test_loss_list = []\n",
        "test_acc_list = []  \n",
        "T = 3\n",
        "\n",
        "ema = EMA(decay = decay, model = student_model)\n",
        "ema.register()\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(n_iters)):\n",
        "  total_samples=0\n",
        "  correct_samples=0\n",
        "  running_loss = 0.0\n",
        "  for step, (image, label) in enumerate(train_loader):\n",
        "    image = image.to(device)\n",
        "    label = label.to(device)\n",
        "    optimizer_teacher.zero_grad()\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      teacher_pred = teacher_model(image)\n",
        "    # print(output.shape)\n",
        "    # print(label.shape)\n",
        "    # loss\n",
        "    student_pred = student_model(image)\n",
        "\n",
        "    # print(type(student_pred))\n",
        "    # print(type(teacher_pred))\n",
        "    student_loss = loss_fun(student_pred, label)\n",
        "\n",
        "\n",
        "    # dest_loss = destillation_loss(student_pred, teacher_pred)\n",
        "    soft_teacher_pred = F.softmax(teacher_pred / temperature, dim=1)\n",
        "    # print(\"Soft teacher : \", soft_teacher_pred)\n",
        "    soft_student_pred = F.softmax(student_pred / temperature, dim=1)\n",
        "    # print(\"Soft student : \", soft_student_pred)\n",
        "    # dest_loss = F.kl_div(soft_teacher_pred.log(), soft_student_pred, reduction='batchmean') * temperature * temperature\n",
        "\n",
        "    dest_loss = nn.KLDivLoss()(F.log_softmax(soft_teacher_pred/T, dim=1),F.softmax(soft_student_pred/T, dim=1)) * T * T \n",
        "    # print(dest_loss)\n",
        "\n",
        "\n",
        "    # print(dest_loss, type(dest_loss))\n",
        "\n",
        "    loss = alpha * student_loss + (1 - alpha) * dest_loss\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    for param in student_model.parameters():\n",
        "        param.grad *= (temperature ** 2)\n",
        "\n",
        "\n",
        "    optimizer_student.step()\n",
        "    ema.update()\n",
        "    _,out=torch.max(student_pred,1)\n",
        "    total_samples+=label.shape[0]\n",
        "    correct_samples+=(out==label).sum().item()\n",
        "    # break\n",
        "\n",
        "  accuracy=(correct_samples/total_samples)*100.0\n",
        "  print(\"Train Epoch: {}/{} | step:{}/{} | Loss:{:.4f} | Accuracy{: .2f}\".format(epoch,n_iters,step,total_steps,loss.item(),accuracy))\n",
        "  loss_list.append(loss.item())\n",
        "  \n",
        "  # update learning rate\n",
        "  scheduler.step()\n",
        "  # validate model\n",
        "  student_model.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          images, labels = data[0].to(device), data[1].to(device)\n",
        "          outputs = student_model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "  acc = 100 * correct / total\n",
        "  print(\"Test Epoch: {}/{} | step:{}/{} | Loss:{:.4f} | Accuracy{: .2f}\".format(epoch,n_iters,step,total_steps,loss.item(),acc))\n",
        "  test_acc_list.append(acc)\n",
        "  test_loss_list.append(running_loss)\n",
        "\n",
        "    \n",
        "# print(\"\\n\\nloss list\",list)"
      ],
      "metadata": {
        "id": "06YBqEb7oWyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(loss_list)\n",
        "plot_graph(acc_list)\n",
        "plot_graph(test_loss_list)\n",
        "plot_graph(test_acc_list)\n"
      ],
      "metadata": {
        "id": "w2tBj9lDFcdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}